# ğŸš€ Multi-AutoML Interface

**A unified interface for experimenting with AutoML, allowing you to compare multiple frameworks (AutoGluon, FLAML, H2O, TPOT) with integrated MLOps via MLflow.**

---

## ğŸ¯ **Overview**

The Multi-AutoML Interface is a web/desktop application that simplifies the use of AutoML frameworks, enabling:

- **Side-by-side comparison** of different AutoML engines
- **Integrated MLOps** with complete tracking via MLflow
- **Unified interface** for training, evaluation, and prediction
- **Flexible deployment** (web, Docker, desktop)
- **Detailed metrics and logging**

---

## âœ¨ **Key Features**

### ğŸ¤– **Supported AutoML Frameworks:**
- **AutoGluon** (Amazon) - Exceptional performance
- **FLAML** (Microsoft) - Fast and efficient
- **H2O AutoML** (Enterprise) - Robust and comprehensive
- **TPOT** (Open Source) - Pipelines generated by Genetic Algorithms

### ğŸ“Š **Integrated MLOps:**
- **Complete MLflow tracking**
- **Automatic Data Lake versioning** with DVC
- **Automatic experiment logging**
- **Centralized model registry**
- **Detailed performance metrics**
- **Artifact management**

### ğŸ–¥ï¸ **Multi-Deploy:**
- **Web interface** (Streamlit)
- **Docker container** (production)
- **Desktop app** (Electron)
- **Hugging Face Spaces** (Live Demo)
- **Local development**

### ğŸ›ï¸ **Advanced Interface:**
- **Upload multiple datasets** (Train, Validation, Test)
- **Advanced parameter configuration**
- **Real-time monitoring**
- **Results visualization**
- **Interactive prediction**

---

## ğŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API    â”‚    â”‚   ML Engines    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Streamlit     â”‚â—„â”€â”€â–ºâ”‚ â€¢ Python         â”‚â—„â”€â”€â–ºâ”‚ â€¢ AutoGluon     â”‚
â”‚ â€¢ Electron      â”‚    â”‚ â€¢ FastAPI        â”‚    â”‚ â€¢ FLAML         â”‚
â”‚ â€¢ React         â”‚    â”‚ â€¢ MLflow         â”‚    â”‚ â€¢ H2O AutoML    â”‚
â”‚ â€¢ Custom UI     â”‚    â”‚ â€¢ Logging        â”‚    â”‚ â€¢ TPOT          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Storage       â”‚    â”‚   Monitoring     â”‚    â”‚   Deployment    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ File System   â”‚    â”‚ â€¢ MLflow UI      â”‚    â”‚ â€¢ Docker Hub    â”‚
â”‚ â€¢ MLflow Artifactsâ”‚  â”‚ â€¢ Logs           â”‚    â”‚ â€¢ GitHub        â”‚
â”‚ â€¢ Model Registryâ”‚    â”‚ â€¢ Metrics        â”‚    â”‚ â€¢ Electron Storeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ **Quick Start**

### ğŸ“‹ **Prerequisites:**
- **Python 3.11+**
- **Node.js 16+** (for desktop app)
- **Java 11+** (for H2O AutoML)
- **Git**

### ğŸ”§ **Installation:**

#### **1. Clone the Repository:**
```bash
git clone https://github.com/PedroM2626/Multi-AutoML-Interface.git
cd Multi-AutoML-Interface
```

#### **2. Python Environment:**
```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### **3. Start MLflow:**
```bash
# Start MLflow server
mlflow server --host 0.0.0.0 --port 5000
```

#### **4. Run the Application:**
```bash
# Option 1: Web interface
streamlit run app.py --server.port 8501

# Option 2: Desktop app (requires Node.js)
npm install && npm run dev

# Option 3: Docker
docker-compose up
```

---

## ğŸ“– **User Guide**

### ğŸ¯ **Basic Workflow:**

#### **1. Data Upload:**
- Supported formats: CSV, Excel
- **Multiple splits supported**: Train (mandatory), Validation (optional), and Test (optional)
- Automatic type detection
- **Automatic Data Lake**: When processing data, it is copied to the `data_lake/` folder and versioned via DVC, generating hashes for version control.

#### **2. Experiment Configuration:**
- **Framework**: AutoGluon, FLAML, H2O, TPOT
- **Target variable**: Target column
- **Advanced parameters**: seed, time limits, folds, max textual features (TF-IDF), CV, etc.

#### **3. Training:**
- **Real-time monitoring**
- **Detailed logs**
- **Progress tracking**

#### **4. Results Analysis:**
- **Comparative leaderboards**
- **Performance metrics**
- **Model insights**

#### **5. Prediction:**
- **Upload new data**
- **Batch prediction**
- **Real-time inference**

---

## ğŸ› ï¸ **Advanced Configuration**

### âš™ï¸ **Framework Parameters:**

#### **AutoGluon:**
```python
{
    'presets': 'best_quality',
    'time_limit': 3600,
    'seed': 42,
    'num_bag_folds': 5,
    'num_bag_sets': 1
}
```

#### **FLAML:**
```python
{
    'time_budget': 3600,
    'seed': 42,
    'ensemble': True,
    'metric': 'accuracy',
    'estimator_list': ['lgbm', 'xgboost', 'rf']
}
```

#### **H2O AutoML:**
```python
{
    'max_runtime_secs': 3600,
    'max_models': 20,
    'seed': 42,
    'nfolds': 5,
    'balance_classes': True,
    'sort_metric': 'AUTO'
}
```

#### **TPOT:**
```python
{
    'generations': 5,
    'population_size': 20,
    'cv': 5,
    'max_time_mins': 30,
    'config_dict': 'TPOT sparse',
    'tfidf_max_features': 500,
    'tfidf_ngram_range': (1, 2)
}
```

### ğŸ›ï¸ **MLflow Configuration:**
```python
# Experiments
mlflow.set_experiment("AutoGluon_Experiments")
mlflow.set_experiment("FLAML_Experiments") 
mlflow.set_experiment("H2O_Experiments")

# Tracking
mlflow.log_param("framework", "autogluon")
mlflow.log_metric("accuracy", 0.95)
mlflow.log_artifact("model.pkl")
```

---

## ğŸ³ **Deploy with Docker**

###  ğŸ“¦ **Build and Run:**

#### **1. Build Image:**
```bash
docker build -t multi-automl:latest .
```

#### **2. Docker Compose:**
```bash
# Start all services
docker-compose up -d

# Logs
docker-compose logs -f

# Stop
docker-compose down
```

#### **3. Ports:**
- **8501**: Streamlit UI
- **5000**: MLflow UI
- **54321**: H2O Cluster

---

## ğŸ–¥ï¸ **Desktop App (Electron)**

### ğŸ“¦ **Installation and Build:**

#### **1. Install Node.js:**
```bash
# Download: https://nodejs.org/
node --version
npm --version
```

#### **2. Install Dependencies:**
```bash
npm install
```

#### **3. Development Mode:**
```bash
npm run dev
```

#### **4. Production Build:**
```bash
# Windows
npm run build-win

# Mac
npm run build-mac

# Linux
npm run build-linux
```

#### **5. Desktop Features:**
- **Native window** (without browser)
- **Professional menu** with shortcuts
- **Native file dialogs**
- **System integration**
- **Offline mode**

---

## ğŸ“Š **Performance and Benchmarks**

### ğŸ† **Framework Comparison:**

| Framework | Speed | Performance | Memory | Ease of Use |
|-----------|-------|-------------|--------|-------------|
| **AutoGluon** | âš¡âš¡âš¡ | ğŸ†ğŸ† | ğŸ†ğŸ† | ğŸ†ğŸ†ğŸ† |
| **FLAML** | âš¡âš¡âš¡âš¡ | ğŸ†ğŸ† | ğŸ†ğŸ†ğŸ† | ğŸ†ğŸ† |
| **H2O** | âš¡âš¡ | ğŸ†ğŸ†ğŸ† | ğŸ† | ğŸ† |
| **TPOT** | âš¡ | ğŸ†ğŸ†ğŸ† | ğŸ†ğŸ† | ğŸ† |

### ğŸ“ˆ **Performance Metrics:**

#### **Test Dataset (10k rows, 50 columns):**
```
AutoGluon: 2.5 min, 94.2% accuracy
FLAML: 1.8 min, 93.8% accuracy  
H2O: 4.2 min, 94.0% accuracy
```

#### **Memory Usage:**
```
AutoGluon: ~2GB RAM
FLAML: ~1.5GB RAM
H2O: ~3GB RAM
TPOT: ~1GB RAM (Optimized)
```

---

## ğŸ”§ **Troubleshooting**

### âŒ **Common Issues:**

#### **"Java not found" (H2O):**
```bash
# Windows: Add JAVA_HOME
set JAVA_HOME="C:\Program Files\Java\jdk-11"

# Mac/Linux: Export variable
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
```

#### **"Port already in use":**
```bash
# Check ports
netstat -an | findstr 8501

# Kill process
taskkill /PID <PID> /F

# Use another port
streamlit run app.py --server.port 8502
```

#### **"Memory error":**
```bash
# Increase H2O memory
export H2O_MAX_MEM_SIZE="8G"

# Or reduce dataset
```

#### **"MLflow connection error" / "Missing mlruns":**
```bash
# In the new version, the mlruns/.trash directory is automatically healed and recreated if broken.
# For other issues:
mlflow server --host 0.0.0.0 --port 5000
```

---

## ğŸ§ª **Testing**

### ğŸ“‹ **Test Suite:**

#### **1. Integration Tests:**
```bash
# Test H2O integration
python tests/test_h2o_integration.py

# Test MLflow integration  
python tests/test_mlflow_integration.py
```

#### **2. Unit Tests:**
```bash
# Test utils
pytest tests/test_utils.py

# Test interface
pytest tests/test_interface.py
```

#### **3. Performance Tests:**
```bash
# Benchmark frameworks
python tests/benchmark_frameworks.py
```

---

## ğŸ“ **Project Structure**

```
Multi-AutoML-Interface/
â”œâ”€â”€ ğŸ“ src/                    # Main source code
â”‚   â”œâ”€â”€ ğŸ“„ autogluon_utils.py  # AutoGluon integration
â”‚   â”œâ”€â”€ ğŸ“„ flaml_utils.py      # FLAML integration
â”‚   â”œâ”€â”€ ğŸ“„ h2o_utils.py        # H2O integration
â”‚   â”œâ”€â”€ ğŸ“„ tpot_utils.py       # TPOT integration 
â”‚   â”œâ”€â”€ ğŸ“„ mlflow_utils.py     # MLflow helpers and auto-healing
â”‚   â”œâ”€â”€ ğŸ“„ mlflow_cache.py     # Cache optimization
â”‚   â”œâ”€â”€ ğŸ“„ data_utils.py       # Data processing
â”‚   â””â”€â”€ ğŸ“„ log_utils.py        # Logging utilities
â”œâ”€â”€ ğŸ“ tests/                  # Automated tests
â”‚   â”œâ”€â”€ ğŸ“„ test_h2o_integration.py
â”‚   â”œâ”€â”€ ğŸ“„ test_mlflow_integration.py
â”‚   â””â”€â”€ ğŸ“„ test_performance.py
â”œâ”€â”€ ğŸ“ electron/               # Desktop app (Electron)
â”‚   â”œâ”€â”€ ğŸ“„ main.js             # Main process
â”‚   â”œâ”€â”€ ğŸ“„ preload.js          # Security bridge
â”‚   â”œâ”€â”€ ğŸ“„ renderer.js         # UI enhancements
â”‚   â””â”€â”€ ğŸ“ assets/             # Icons and resources
â”œâ”€â”€ ğŸ“„ app.py                  # Streamlit main app
â”œâ”€â”€ ğŸ“„ requirements.txt        # Python dependencies
â”œâ”€â”€ ğŸ“„ package.json            # Node.js dependencies
â”œâ”€â”€ ğŸ³ Dockerfile              # Docker configuration
â”œâ”€â”€ ğŸ³ docker-compose.yml      # Multi-service setup
â””â”€â”€ ğŸ“„ README.md               # This file
```

---

## ğŸ¤ **Contributing**

### ğŸ¯ **How to Contribute:**

#### **1. Fork and Clone:**
```bash
git clone https://github.com/PedroM2626/Multi-AutoML-Interface.git
cd Multi-AutoML-Interface
```

#### **2. Create Branch:**
```bash
git checkout -b feature/new-feature
```

#### **3. Develop:**
- Follow existing code style
- Add tests
- Document changes

#### **4. Commit and Push:**
```bash
git add .
git commit -m "feat: add new feature"
git push origin feature/new-feature
```

#### **5. Pull Request:**
- Describe changes
- Link issues
- Await review

### ğŸ“ **Guidelines:**
- **Python**: PEP 8
- **JavaScript**: ESLint
- **Commits**: Conventional Commits
- **Docs**: Clear Markdown

---

## ğŸ“„ **License**

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ **Credits and Acknowledgements**

### ğŸ¤– **Frameworks:**
- **AutoGluon** - Amazon Web Services
- **FLAML** - Microsoft Research  
- **H2O AutoML** - H2O.ai
- **TPOT** - Rhodes Lab
- **MLflow** - Databricks

### ğŸ› ï¸ **Technologies:**
- **Streamlit** - Web interface
- **Electron** - Desktop app
- **Docker** - Containerization
- **FastAPI** - Backend API

### ğŸ“š **Resources:**
- **AutoML Documentation**
- **MLflow Tracking**
- **Streamlit Components**
- **Electron Security**

---

## ğŸ—ºï¸ **Future Roadmap**

### ğŸš€ **Upcoming Features**
- [ ] **Auto-sklearn** (meta-learning)
- [ ] **Model explainability** (SHAP, LIME)
- [ ] **Advanced visualizations**
- [ ] **Batch processing**

---

### ğŸŒ **Live Demo:**
[Hugging Face Spaces - Multi-AutoML Interface](https://huggingface.co/spaces/PedroM2626/Multi-AutoML-Interface)

---

## ğŸ‰ **Conclusion**

The **Multi-AutoML Interface** represents a complete and professional solution for AutoML experimentation, combining:

- **ğŸ¤– Multiple frameworks** in a unified interface
- **ğŸ“Š Integrated MLOps** with full tracking
- **ğŸ–¥ï¸ Flexible deployment** (web, desktop, container)
- **ğŸ›ï¸ Intuitive interface** for technical users
- **ğŸ”§ Advanced configuration** for experts
- **ğŸ“ˆ Optimized performance** for production

**Ideal for:**
- **Data Scientists** wanting to compare frameworks
- **Researchers** experimenting with different approaches
- **Students** learning about AutoML

---

*Developed by Pedro Morato Lahoz*