# ğŸš€ Multi-AutoML Interface

**Uma interface unificada para experimentaÃ§Ã£o com AutoML, permitindo comparar mÃºltiplos frameworks (AutoGluon, FLAML, H2O) com MLOps integrado via MLflow.**

---

## ğŸ¯ **VisÃ£o Geral**

O Multi-AutoML Interface Ã© uma aplicaÃ§Ã£o web/desktop que simplifica o uso de frameworks AutoML, permitindo:

- **ComparaÃ§Ã£o lado a lado** de diferentes engines AutoML
- **MLOps integrado** com tracking completo via MLflow
- **Interface unificada** para treinamento, avaliaÃ§Ã£o e prediÃ§Ã£o
- **Deploy flexÃ­vel** (web, Docker, desktop)
- **MÃ©tricas e logging** detalhados

---

## âœ¨ **Features Principais**

### ğŸ¤– **Frameworks AutoML Suportados:**
- **AutoGluon** (Amazon) - Performance excepcional
- **FLAML** (Microsoft) - Veloz e eficiente
- **H2O AutoML** (Enterprise) - Robusto e completo
- **TPOT** (Open Source) - Pipelines gerados por Algoritmos GenÃ©ticos

### ğŸ“Š **MLOps Integrado:**
- **MLflow tracking** completo
- **Data Lake versioning** automÃ¡tico com DVC
- **Experiment logging** automÃ¡tico
- **Model registry** centralizado
- **Performance metrics** detalhadas
- **Artifact management**

### ğŸ–¥ï¸ **Multi-Deploy:**
- **Web interface** (Streamlit)
- **Docker container** (produÃ§Ã£o)
- **Desktop app** (Electron)
- **Hugging Face Spaces** (Live Demo)
- **Local development**

### ğŸ›ï¸ **Interface AvanÃ§ada:**
- **Upload de mÃºltiplos datasets** (Treino, ValidaÃ§Ã£o, Teste)
- **ConfiguraÃ§Ã£o avanÃ§ada** de parÃ¢metros
- **Monitoramento em tempo real**
- **VisualizaÃ§Ã£o de resultados**
- **PrediÃ§Ã£o interativa**

---

## ğŸ—ï¸ **Arquitetura**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API    â”‚    â”‚   ML Engines    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Streamlit     â”‚â—„â”€â”€â–ºâ”‚ â€¢ Python         â”‚â—„â”€â”€â–ºâ”‚ â€¢ AutoGluon     â”‚
â”‚ â€¢ Electron      â”‚    â”‚ â€¢ FastAPI        â”‚    â”‚ â€¢ FLAML         â”‚
â”‚ â€¢ React         â”‚    â”‚ â€¢ MLflow         â”‚    â”‚ â€¢ H2O AutoML    â”‚
â”‚ â€¢ Custom UI     â”‚    â”‚ â€¢ Logging        â”‚    â”‚ â€¢ TPOT          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Storage       â”‚    â”‚   Monitoring     â”‚    â”‚   Deployment    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ File System   â”‚    â”‚ â€¢ MLflow UI      â”‚    â”‚ â€¢ Docker Hub    â”‚
â”‚ â€¢ MLflow Artifactsâ”‚  â”‚ â€¢ Logs           â”‚    â”‚ â€¢ GitHub        â”‚
â”‚ â€¢ Model Registryâ”‚    â”‚ â€¢ Metrics        â”‚    â”‚ â€¢ Electron Storeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ **Quick Start**

### ğŸ“‹ **PrÃ©-requisitos:**
- **Python 3.11+**
- **Node.js 16+** (para desktop app)
- **Java 11+** (para H2O AutoML)
- **Git**

### ğŸ”§ **InstalaÃ§Ã£o:**

#### **1. Clonar RepositÃ³rio:**
```bash
git clone https://github.com/PedroM2626/Multi-AutoML-Interface.git
cd Multi-AutoML-Interface
```

#### **2. Ambiente Python:**
```bash
# Criar ambiente virtual
python -m venv venv

# Ativar (Windows)
venv\Scripts\activate

# Ativar (Mac/Linux)
source venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

#### **3. Iniciar MLflow:**
```bash
# Iniciar MLflow server
mlflow server --host 0.0.0.0 --port 5000
```

#### **4. Rodar AplicaÃ§Ã£o:**
```bash
# OpÃ§Ã£o 1: Web interface
streamlit run app.py --server.port 8501

# OpÃ§Ã£o 2: Desktop app (requer Node.js)
npm install && npm run dev

# OpÃ§Ã£o 3: Docker
docker-compose up
```

---

## ğŸ“– **Guia de Uso**

### ğŸ¯ **Workflow BÃ¡sico:**

#### **1. Upload de Dados:**
- Formatos suportados: CSV, Excel
- **MÃºltiplos splits suportados**: Treino (obrigatÃ³rio), ValidaÃ§Ã£o (opcional) e Teste (opcional)
- DetecÃ§Ã£o automÃ¡tica de tipos
- **Data Lake AutomÃ¡tico**: Ao processar os dados, sÃ£o copiados para a pasta `data_lake/` e versionados via DVC, com Hashes gerados para controle de versionamento.

#### **2. ConfiguraÃ§Ã£o do Experimento:**
- **Framework**: AutoGluon, FLAML, H2O, TPOT
- **Target variable**: Coluna alvo
- **ParÃ¢metros avanÃ§ados**: seed, tempo, folds, max features textuais (TF-IDF), CV, etc.

#### **3. Treinamento:**
- **Monitoramento em tempo real**
- **Logs detalhados**
- **Progress tracking**

#### **4. AnÃ¡lise de Resultados:**
- **Leaderboards** comparativos
- **Performance metrics**
- **Model insights**

#### **5. PrediÃ§Ã£o:**
- **Upload de novos dados**
- **Batch prediction**
- **Real-time inference**

---

## ğŸ› ï¸ **ConfiguraÃ§Ã£o AvanÃ§ada**

### âš™ï¸ **ParÃ¢metros dos Frameworks:**

#### **AutoGluon:**
```python
{
    'presets': 'best_quality',
    'time_limit': 3600,
    'seed': 42,
    'num_bag_folds': 5,
    'num_bag_sets': 1
}
```

#### **FLAML:**
```python
{
    'time_budget': 3600,
    'seed': 42,
    'ensemble': True,
    'metric': 'accuracy',
    'estimator_list': ['lgbm', 'xgboost', 'rf']
}
```

#### **H2O AutoML:**
```python
{
    'max_runtime_secs': 3600,
    'max_models': 20,
    'seed': 42,
    'nfolds': 5,
    'balance_classes': True,
    'sort_metric': 'AUTO'
}
```

#### **TPOT:**
```python
{
    'generations': 5,
    'population_size': 20,
    'cv': 5,
    'max_time_mins': 30,
    'config_dict': 'TPOT sparse',
    'tfidf_max_features': 500,
    'tfidf_ngram_range': (1, 2)
}
```

### ğŸ›ï¸ **ConfiguraÃ§Ã£o MLflow:**
```python
# Experiments
mlflow.set_experiment("AutoGluon_Experiments")
mlflow.set_experiment("FLAML_Experiments") 
mlflow.set_experiment("H2O_Experiments")

# Tracking
mlflow.log_param("framework", "autogluon")
mlflow.log_metric("accuracy", 0.95)
mlflow.log_artifact("model.pkl")
```

---

## ğŸ³ **Deploy com Docker**

### ğŸ“¦ **Build e ExecuÃ§Ã£o:**

#### **1. Build da Imagem:**
```bash
docker build -t multi-automl:latest .
```

#### **2. Docker Compose:**
```bash
# Iniciar todos os serviÃ§os
docker-compose up -d

# Logs
docker-compose logs -f

# Parar
docker-compose down
```

#### **3. Portas:**
- **8501**: Streamlit UI
- **5000**: MLflow UI
- **54321**: H2O Cluster

---

## ğŸ–¥ï¸ **Desktop App (Electron)**

### ğŸ“¦ **InstalaÃ§Ã£o e Build:**

#### **1. Instalar Node.js:**
```bash
# Download: https://nodejs.org/
node --version
npm --version
```

#### **2. Instalar DependÃªncias:**
```bash
npm install
```

#### **3. Modo Desenvolvimento:**
```bash
npm run dev
```

#### **4. Build para ProduÃ§Ã£o:**
```bash
# Windows
npm run build-win

# Mac
npm run build-mac

# Linux
npm run build-linux
```

#### **5. Features Desktop:**
- **Janela nativa** (sem navegador)
- **Menu profissional** com atalhos
- **File dialogs** nativos
- **System integration**
- **Offline mode**

---

## ğŸ“Š **Performance e Benchmarks**

### ğŸ† **ComparaÃ§Ã£o de Frameworks:**

| Framework | Velocidade | Performance | MemÃ³ria | Facilidade |
|-----------|------------|--------------|---------|------------|
| **AutoGluon** | âš¡âš¡âš¡ | ğŸ†ğŸ† | ğŸ†ğŸ† | ğŸ†ğŸ†ğŸ† |
| **FLAML** | âš¡âš¡âš¡âš¡ | ğŸ†ğŸ† | ğŸ†ğŸ†ğŸ† | ğŸ†ğŸ† |
| **H2O** | âš¡âš¡ | ğŸ†ğŸ†ğŸ† | ğŸ† | ğŸ† |
| **TPOT** | âš¡ | ğŸ†ğŸ†ğŸ† | ğŸ†ğŸ† | ğŸ† |

### ğŸ“ˆ **MÃ©tricas de Performance:**

#### **Dataset Teste (10k linhas, 50 colunas):**
```
AutoGluon: 2.5 min, 94.2% accuracy
FLAML: 1.8 min, 93.8% accuracy  
H2O: 4.2 min, 94.0% accuracy
```

#### **Uso de MemÃ³ria:**
```
AutoGluon: ~2GB RAM
FLAML: ~1.5GB RAM
H2O: ~3GB RAM
TPOT: ~1GB RAM (Otimizado)
```

---

## ğŸ”§ **Troubleshooting**

### âŒ **Problemas Comuns:**

#### **"Java nÃ£o encontrado" (H2O):**
```bash
# Windows: Adicionar JAVA_HOME
set JAVA_HOME="C:\Program Files\Java\jdk-11"

# Mac/Linux: Exportar variÃ¡vel
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
```

#### **"Porta jÃ¡ em uso":**
```bash
# Verificar portas
netstat -an | findstr 8501

# Matar processo
taskkill /PID <PID> /F

# Usar outra porta
streamlit run app.py --server.port 8502
```

#### **"Memory error":**
```bash
# Aumentar memÃ³ria H2O
export H2O_MAX_MEM_SIZE="8G"

# Ou reduzir dataset
```

#### **"MLflow connection error" / "Missing mlruns":**
```bash
# Na nova versÃ£o, o diretÃ³rio mlruns/.trash Ã© cicatrizado e recriado automaticamente caso seja rompido.
# Para outros problemas:
mlflow server --host 0.0.0.0 --port 5000
```

---

## ğŸ§ª **Testes**

### ğŸ“‹ **Suite de Testes:**

#### **1. Testes de IntegraÃ§Ã£o:**
```bash
# Testar H2O integration
python tests/test_h2o_integration.py

# Testar MLflow integration  
python tests/test_mlflow_integration.py
```

#### **2. Testes UnitÃ¡rios:**
```bash
# Testar utils
pytest tests/test_utils.py

# Testar interface
pytest tests/test_interface.py
```

#### **3. Testes de Performance:**
```bash
# Benchmark frameworks
python tests/benchmark_frameworks.py
```

---

## ğŸ“ **Estrutura do Projeto**

```
Multi-AutoML-Interface/
â”œâ”€â”€ ğŸ“ src/                    # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ğŸ“„ autogluon_utils.py  # AutoGluon integration
â”‚   â”œâ”€â”€ ğŸ“„ flaml_utils.py      # FLAML integration
â”‚   â”œâ”€â”€ ğŸ“„ h2o_utils.py        # H2O integration
â”‚   â”œâ”€â”€ ğŸ“„ tpot_utils.py       # TPOT integration 
â”‚   â”œâ”€â”€ ğŸ“„ mlflow_utils.py     # MLflow helpers e auto-healing
â”‚   â”œâ”€â”€ ğŸ“„ mlflow_cache.py     # Cache otimizado
â”‚   â”œâ”€â”€ ğŸ“„ data_utils.py       # Data processing
â”‚   â””â”€â”€ ğŸ“„ log_utils.py        # Logging utilities
â”œâ”€â”€ ğŸ“ tests/                  # Testes automatizados
â”‚   â”œâ”€â”€ ğŸ“„ test_h2o_integration.py
â”‚   â”œâ”€â”€ ğŸ“„ test_mlflow_integration.py
â”‚   â””â”€â”€ ğŸ“„ test_performance.py
â”œâ”€â”€ ğŸ“ electron/               # Desktop app (Electron)
â”‚   â”œâ”€â”€ ğŸ“„ main.js             # Main process
â”‚   â”œâ”€â”€ ğŸ“„ preload.js          # Security bridge
â”‚   â”œâ”€â”€ ğŸ“„ renderer.js         # UI enhancements
â”‚   â””â”€â”€ ğŸ“ assets/             # Icons e recursos
â”œâ”€â”€ ğŸ“„ app.py                  # Streamlit main app
â”œâ”€â”€ ğŸ“„ requirements.txt        # Python dependencies
â”œâ”€â”€ ğŸ“„ package.json            # Node.js dependencies
â”œâ”€â”€ ğŸ³ Dockerfile              # Docker configuration
â”œâ”€â”€ ğŸ³ docker-compose.yml      # Multi-service setup
â””â”€â”€ ğŸ“„ README.md               # Este arquivo
```

---

## ğŸ¤ **ContribuiÃ§Ã£o**

### ğŸ¯ **Como Contribuir:**

#### **1. Fork e Clone:**
```bash
git clone https://github.com/PedroM2626/Multi-AutoML-Interface.git
cd Multi-AutoML-Interface
```

#### **2. Criar Branch:**
```bash
git checkout -b feature/nova-feature
```

#### **3. Desenvolver:**
- Seguir cÃ³digo style existente
- Adicionar testes
- Documentar mudanÃ§as

#### **4. Commit e Push:**
```bash
git add .
git commit -m "feat: add nova feature"
git push origin feature/nova-feature
```

#### **5. Pull Request:**
- Descrever mudanÃ§as
- Linkar issues
- Aguardar review

### ğŸ“ **Guidelines:**
- **Python**: PEP 8
- **JavaScript**: ESLint
- **Commits**: Conventional Commits
- **Docs**: Markdown claro

---

## ğŸ“„ **LicenÃ§a**

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## ğŸ™ **CrÃ©ditos e Agradecimentos**

### ğŸ¤– **Frameworks:**
- **AutoGluon** - Amazon Web Services
- **FLAML** - Microsoft Research  
- **H2O AutoML** - H2O.ai
- **TPOT** - Rhodes Lab
- **MLflow** - Databricks

### ğŸ› ï¸ **Tecnologias:**
- **Streamlit** - Interface web
- **Electron** - Desktop app
- **Docker** - ContainerizaÃ§Ã£o
- **FastAPI** - Backend API

### ğŸ“š **Recursos:**
- **AutoML Documentation**
- **MLflow Tracking**
- **Streamlit Components**
- **Electron Security**

---

## ğŸ—ºï¸ **Roadmap Futuro**

### ğŸš€ **PrÃ³ximas Features**
- [ ] **Auto-sklearn** (meta-learning)
- [ ] **Model explainability** (SHAP, LIME)
- [ ] **Advanced visualizations**
- [ ] **Batch processing**

---

### ğŸŒ **Live Demo:**
[Hugging Face Spaces - Multi-AutoML Interface](https://huggingface.co/spaces/PedroM2626/Multi-AutoML-Interface)

---

## ğŸ‰ **ConclusÃ£o**

O **Multi-AutoML Interface** representa uma soluÃ§Ã£o completa e profissional para experimentaÃ§Ã£o com AutoML, combinando:

- **ğŸ¤– MÃºltiplos frameworks** em uma interface unificada
- **ğŸ“Š MLOps integrado** com tracking completo
- **ğŸ–¥ï¸ Deploy flexÃ­vel** (web, desktop, container)
- **ğŸ›ï¸ Interface intuitiva** para usuÃ¡rios tÃ©cnicos
- **ğŸ”§ ConfiguraÃ§Ã£o avanÃ§ada** para experts
- **ğŸ“ˆ Performance otimizada** para produÃ§Ã£o

**Ideal para:**
- **Data Scientists** que querem comparar frameworks
- **Pesquisadores** que experimentam diferentes abordagens
- **Estudantes** que aprendem sobre AutoML

---

*Desenvolvido por Pedro Morato Lahoz*